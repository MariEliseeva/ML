{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_int(label):\n",
    "    labels = {'A' : 0, 'B' : 1, 'C' : 2, 'D' : 3, 'E' : 4, 'F' : 5, 'G' : 6, 'H' : 7, 'I' : 8, 'J' : 9}\n",
    "    return labels[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def split(images, labels):\n",
    "    test_indexes = random.sample(range(len(images)), len(images) // 10)\n",
    "    train_indexes = set([i for i in range(len(images))]) - set(test_indexes)\n",
    "    return [images[i] for i in train_indexes], [labels[i] for i in train_indexes], [images[i] for i in test_indexes], [labels[i] for i in test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_points():\n",
    "    images = []\n",
    "    labels = []\n",
    "    for path, subdirs, files in os.walk(\"../Datasets/notMNIST_small\"):\n",
    "        for name in files:\n",
    "            try:\n",
    "                Image.open(os.path.join(path, name), 'r')\n",
    "                images.append(os.path.join(path, name))\n",
    "                labels.append(label_to_int(path.split(\"/\")[-1]))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    train_images, train_labels, test_images, test_labels = split(images, labels)\n",
    "    train_points = []\n",
    "    new_train_labels = []\n",
    "    test_points = []\n",
    "    new_test_labels = []\n",
    "    \n",
    "    for j in range(len(train_images)):\n",
    "        try:\n",
    "            im = Image.open(train_images[j], 'r')\n",
    "            im1 = im.rotate(10)\n",
    "            im2 = im.rotate(-10)\n",
    "            width, height = im.size\n",
    "            im3 = im.resize((width - 10, height - 10)).resize((width, height))\n",
    "            pixel_values = list(im.getdata())\n",
    "            pixel_values1 = list(im1.getdata())\n",
    "            pixel_values2 = list(im2.getdata())\n",
    "            pixel_values3 = list(im3.getdata())\n",
    "            for i in range(len(pixel_values)):\n",
    "                pixel_values[i] = pixel_values[i] * 1.0 / 255\n",
    "            for i in range(len(pixel_values1)):\n",
    "                pixel_values1[i] = pixel_values1[i] * 1.0 / 255\n",
    "            for i in range(len(pixel_values2)):\n",
    "                pixel_values2[i] = pixel_values2[i] * 1.0 / 255\n",
    "            for i in range(len(pixel_values3)):\n",
    "                pixel_values3[i] = pixel_values3[i] * 1.0 / 255\n",
    "            train_points.append(np.reshape(pixel_values, [width, height]))\n",
    "            train_points.append(np.reshape(pixel_values1, [width, height]))\n",
    "            train_points.append(np.reshape(pixel_values2, [width, height]))\n",
    "            train_points.append(np.reshape(pixel_values3, [width, height]))\n",
    "            new_train_labels.append(train_labels[j])\n",
    "            new_train_labels.append(train_labels[j])\n",
    "            new_train_labels.append(train_labels[j])\n",
    "            new_train_labels.append(train_labels[j])\n",
    "        except:\n",
    "            pass \n",
    "\n",
    "    train_points, train_labels = np.array(train_points), np.array(new_train_labels)\n",
    "    permutation = np.arange(train_points.shape[0])\n",
    "    np.random.shuffle(permutation)\n",
    "    train_points, train_labels = train_points[permutation], train_labels[permutation]\n",
    "    \n",
    "    for j in range(len(test_images)):\n",
    "        try:\n",
    "            im = Image.open(test_images[j], 'r')\n",
    "            width, height = im.size\n",
    "            pixel_values = list(im.getdata())\n",
    "            for i in range(len(pixel_values)):\n",
    "                pixel_values[i] = pixel_values[i] * 1.0 / 255\n",
    "            test_points.append(np.reshape(pixel_values, [width, height]))\n",
    "            new_test_labels.append(test_labels[j])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    test_points, test_labels = np.array(test_points), np.array(new_test_labels)\n",
    "    return train_points, train_labels, test_points, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_points, train_labels, test_points, test_labels = read_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2106.5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_points) * 1.0 / 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features, labels, mode):\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "    conv1 = keras.layers.Conv2D(8, [3, 3], padding=\"same\", activation=tf.nn.relu)(input_layer)\n",
    "    conv2 = keras.layers.Conv2D(8, [3, 3], padding=\"same\", activation=tf.nn.relu)(conv1)\n",
    "    conv3 = keras.layers.Conv2D(8, [3, 3], padding=\"same\", activation=tf.nn.relu)(conv2)\n",
    "    conv4 = keras.layers.Conv2D(8, [3, 3], padding=\"same\", activation=tf.nn.relu)(conv3)\n",
    "    \n",
    "    conv4_reshaped = tf.reshape(conv4, [-1, 28 * 28 * 8])\n",
    "    \n",
    "    fc1 = keras.layers.Dense(64, activation=tf.tanh)(conv4_reshaped)\n",
    "    fc2 = keras.layers.Dense(64, activation=tf.tanh)(fc1)\n",
    "    logits = keras.layers.Dense(10)(fc2)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1)\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(train_points, train_labels, validate_points, validate_labels, dir_name, N):\n",
    "    mnist_classifier = tf.estimator.Estimator(\n",
    "        model_fn=model, model_dir=dir_name)\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": train_points},\n",
    "        y=train_labels,\n",
    "        batch_size=32,\n",
    "        num_epochs=None,\n",
    "        shuffle=True)\n",
    "\n",
    "    for epoch in range(N):\n",
    "        mnist_classifier.train(input_fn=train_input_fn, steps=1872)\n",
    "        eval_input_fn_train = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": train_points},\n",
    "            y=train_labels,\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "        eval_results_train = mnist_classifier.evaluate(input_fn=eval_input_fn_train)\n",
    "        \n",
    "        eval_input_fn_test = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": test_points},\n",
    "            y=test_labels,\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "        eval_results_test = mnist_classifier.evaluate(input_fn=eval_input_fn_test)\n",
    "        print(\"Epoch \" + str(epoch) + \": train accuracy = \" + str(eval_results_train[\"accuracy\"]) + \", test accuracy = \" + str(eval_results_test[\"accuracy\"]))\n",
    "        if eval_results_test[\"accuracy\"] >= 0.95:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train accuracy = 0.93454784, test accuracy = 0.9230769\n",
      "Epoch 1: train accuracy = 0.94610435, test accuracy = 0.9257479\n"
     ]
    }
   ],
   "source": [
    "train_and_validate(train_points, train_labels, test_points, test_labels,\"not_mnist_model\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
